{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "url = \"\"\"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\"\"\n",
        "if not os.path.exists(url.split(\"/\")[-1]):\n",
        "    urlretrieve(url, url.split(\"/\")[-1])\n",
        "    print(\"Downloaded\", url)"
      ],
      "metadata": {
        "id": "wGF8BgJvO5hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e5fa7ff-3a86-4025-d075-3303744c7bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "OSBuEFWiOqRH",
        "outputId": "cb9eea6d-8470-48d4-8740-f1cdcbc1d412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:\tReg.dampening:\tTraining set accuracy:\n",
            "0.03071\t\t0.00977\t\t100.0%\n",
            "0.09655\t\t0.00099\t\t100.0%\n",
            "0.00099\t\t3.00000\t\t99.9%\n",
            "0.30353\t\t0.95425\t\t50.0%\n",
            "0.00099\t\t0.03071\t\t100.0%\n",
            "3.00000\t\t0.95425\t\t50.0%\n",
            "0.00311\t\t0.00311\t\t100.0%\n",
            "0.00010\t\t0.00977\t\t87.0%\n",
            "0.03071\t\t0.30353\t\t99.9%\n",
            "0.09655\t\t0.00311\t\t100.0%\n",
            "Best parameters: 0.03071, 0.00977\n",
            "Test set accuracy 85.8%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI have learned more about numpy and how to get a basic model strcutre where we can train and evaluate it.\\nThe most challenging part in this assignment was how to implement objective function and gredient descent with vectorized data.\\nIt took alot of time to figure it out to plug the vectorized data into different formula to get the correct calculations.\\n\\nWhen I tested my model with the best learning rate and reguliser dampening given after trainig the model 10 times,\\nI got 54.8% accuracy which means that my model manganged to classify the data about 55% correctly.\\nThus, it is hard to say the model is good or not as a classifier but it show that it works as a classifier.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import tarfile\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "\n",
        "# model\n",
        "\n",
        "class feature_transform:\n",
        "  \"\"\"\n",
        "  Transform each document array containing words to binary vectors array using the vocabulary where words from all documents are stored.\n",
        "  If words are found in the vocabulary, vector element will be 1 otherwise 0.\n",
        "  \"\"\"\n",
        "  def __init__(self, vocabulary):\n",
        "    self.vocabulary = vocabulary\n",
        "\n",
        "  def fit_transform(self, X):\n",
        "    X_raw = X\n",
        "    X = np.zeros((2000, len(self.vocabulary)))\n",
        "    for i in range(len(X_raw)):\n",
        "      for token in set(X_raw[i].split()):\n",
        "        X[i, self.vocabulary[token]] = 1\n",
        "    return X\n",
        "\n",
        "class model:\n",
        "  def __init__(self, learning_rate, reguliser_dampening, max_iteration):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.reguliser_dampening = reguliser_dampening\n",
        "    self.max_iteration = max_iteration\n",
        "    self.para = np.random.normal(size=X.shape[1])\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    The model get trained maximum 500 times but it can be interrupted before the maximum time if the model have found an optimized parameter(weight).\n",
        "    Training runs by initializing new parameter using gredient descent and returns the distance between the previous and new parameter vectors.\n",
        "    If the distance between the previous and new parameter vectors is less than 0.001, the training stops.\n",
        "    \"\"\"\n",
        "    for c in range(self.max_iteration):\n",
        "      sum = np.zeros(X.shape[1])\n",
        "      for i in range(len(X)):\n",
        "        con = y[i]*np.dot(self.para,X[i])\n",
        "        if con < 1:\n",
        "          sum += -y[i]*X[i]\n",
        "      g = self.reguliser_dampening*self.para + sum\n",
        "      old_para = self.para.copy()\n",
        "      self.para = self.para - self.learning_rate*g\n",
        "      d = np.linalg.norm([old_para[j] - self.para[j] for j in range(len(self.para))])\n",
        "      if d < 0.001:\n",
        "        break\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predict poitive or negative label (1 or -1) at each document using hyperplane.\n",
        "    \"\"\"\n",
        "    hyperplane = np.dot(X, self.para)\n",
        "    pred_y = np.sign(hyperplane)\n",
        "    return pred_y\n",
        "\n",
        "  def score(self, X, y):\n",
        "    \"\"\"\n",
        "    Score fundtion can be used for getting a stop criterion by calculating the loss of parameters.\n",
        "    I don't use it in my model becasue I use distance between two parameter vectors as the stop criterion.\n",
        "    However, I implemented this part to compare two stop criteria when working on the assignment.\n",
        "    \"\"\"\n",
        "    sum = []\n",
        "    r = self.reguliser_dampening*np.sum(self.para**2)/2\n",
        "    for i in range(len(X)):\n",
        "      sum.append(max(0, 1-y[i]*np.dot(self.para, X[i])))\n",
        "    loss = r + np.sum(np.array(sum))\n",
        "    return loss\n",
        "\n",
        "# Read data files and creat initial X, y dataset.\n",
        "tar = tarfile.open('review_polarity.tar.gz')\n",
        "X_raw = []\n",
        "y = []\n",
        "for member in tar.getnames():\n",
        "    f=tar.extractfile(member).read()\n",
        "    if \"neg\" in member.split('/'):\n",
        "      y.append(-1)\n",
        "      X_raw.append(f.decode())\n",
        "    elif \"pos\" in member.split('/'):\n",
        "      y.append(1)\n",
        "      X_raw.append(f.decode())\n",
        "y = np.array(y)\n",
        "\n",
        "# Creat a vocabulary containing all words from X dataset.\n",
        "ordered_vocabulary = []\n",
        "for x in X_raw:\n",
        "  tokens = x.split()\n",
        "  for word in tokens:\n",
        "    ordered_vocabulary.append(word)\n",
        "\n",
        "ordered_vocabulary = set(ordered_vocabulary)\n",
        "\n",
        "vocabulary = dict()\n",
        "for i, word in enumerate(ordered_vocabulary):\n",
        "  vocabulary[word] = i\n",
        "\n",
        "# Run the class for transforming X data.\n",
        "my_feature = feature_transform(vocabulary)\n",
        "X = my_feature.fit_transform(X_raw)\n",
        "\n",
        "# Devide X, y dataset for train and test (8:2, classes are balanced).\n",
        "X_train, X_test, y_train, y_test = np.concatenate((X[:800],X[1000:1800])), np.concatenate((X[800:1000],X[1800:])), np.concatenate((y[:800],y[1000:1800])), np.concatenate((y[800:1000],y[1800:]))\n",
        "\n",
        "# Find the best learning rate and reguliser_dampening\n",
        "parameter_distribution = {'learning_rate': np.exp(np.linspace(np.log(0.0001), np.log(3), 10)),\n",
        "                          'reguliser_dampening': np.exp(np.linspace(np.log(0.0001), np.log(3), 10))}\n",
        "\n",
        "best_hyperparameters = None\n",
        "print(\"Learning rate:\\tReg.dampening:\\tTraining set accuracy:\")\n",
        "\n",
        "for hyperparameters in ParameterSampler(parameter_distribution, n_iter=10):\n",
        "\n",
        "  learning_rate = hyperparameters['learning_rate']\n",
        "  reguliser_dampening = hyperparameters['reguliser_dampening']\n",
        "  max_iteration = 500\n",
        "  train_model = model(learning_rate, reguliser_dampening, max_iteration)\n",
        "\n",
        "  # Train the model\n",
        "  train_model.fit(X_train, y_train)\n",
        "\n",
        "  # Calculate the training accuracy\n",
        "  training_accuracy = np.sum(train_model.predict(X_train)==y_train)/len(y_train)\n",
        "\n",
        "  # Store the hyperparameters and training accuracy\n",
        "  if best_hyperparameters is None or best_hyperparameters[1] < training_accuracy:\n",
        "    best_hyperparameters = (hyperparameters, training_accuracy)\n",
        "  print(\"%.5f\\t\\t%.5f\\t\\t%.1f%%\" % (hyperparameters['learning_rate'], hyperparameters['reguliser_dampening'], 100*training_accuracy))\n",
        "\n",
        "best_learning_rate = best_hyperparameters[0]['learning_rate']\n",
        "best_reguliser_dampening = best_hyperparameters[0]['reguliser_dampening']\n",
        "print(\"Best parameters: %.5f, %.5f\" % (best_learning_rate, best_reguliser_dampening))\n",
        "\n",
        "# Test model with the best hyperparameters.\n",
        "test_model = model(best_learning_rate, best_reguliser_dampening, max_iteration)\n",
        "test_model.fit(X_train, y_train)\n",
        "test_accuracy = np.sum(test_model.predict(X_test)==y_test)/len(y_test)\n",
        "\n",
        "print(\"Test set accuracy %.1f%%\" % (100*test_accuracy))\n",
        "\n",
        "\"\"\"\n",
        "I have learned more about Numpy and how to get a basic model strcutre where we can train and evaluate it.\n",
        "The most challenging part in this assignment was how to implement objective function and gredient descent with vectorized data.\n",
        "It took alot of time to figure it out to plug the vectorized data into different formula to get the correct calculations.\n",
        "\n",
        "When I tested my model with the best learning rate and reguliser dampening given datasets,\n",
        "I got 85.8% accuracy which means that my model manganged to classify the data about 86% correctly.\n",
        "This result shows that the model learned in training given data to perform as a classifier with 85% accuracy.\n",
        "\"\"\""
      ]
    }
  ]
}